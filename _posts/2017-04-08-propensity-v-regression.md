---
layout: post
title: Limits of propensity score matching
date: 2017-04-08
tag: 
   - ModellingStrategy
   - R
description: Propensity score matching or weighting is an ok family of techniques but isn't a magic bullet
image: /img/0090.svg
socialimage: http://ellisp.github.io/img/0085-gam-vote-predictions.png
category: R
---

This post jots down some playing around with the pros, cons and limits of propensity score matching or weighting for causal social science research.  I was learning as I was going in preparing this, and may have omitted something important.  Today's is a longish post because I wanted to get most my thoughts on this method out of the way.  

## Intro to propensity score matching
One is often faced with an analytical question about causality and effect sizes when the only data around is from a  quasi-experiment, not the random controlled trial one would hope for.  This is common in many fields, but some of the most important occurrences are in public policy.  For example, government programs to help individuals or firms are typically not allocated at random, but go to those with higher need, or higher potential to make something out of the assistance.  This makes isolating the effect of the treatment difficult, to say the least.

A frequently-used family of analytical methods to deal with this are grouped under [propensity score matching](https://en.wikipedia.org/wiki/Propensity_score_matching) (although not all these methods literally "match").  Such methods model the probability of each unit (eg individual or firm) receiving the treatment; and then using these predicted probabilities (called "propensities" for some historical reason or other) to somehow balance the sample to make up for the confounding of the treatment with the other variablers of interest.  

The simplest variant to explain (but definitely not the best) involves creating a control group from the non-treatment individuals that resembles the treatment group in that they have similar propensities to get the treatment, but differs in that the just didn't get it.  Then analysis can proceed "as though" the data had been generated by an experiment.  This approach is easy to explain to non-statisticians and has the great benefit of creating a tangible, concrete "control" group.

But everything depends on the model of the probabilities of getting the treatment. If it's a good model, you're fine.  If not - and in particular, if the chance of getting the treatment is related to unobserved variables which also impact on your response variable of interest - the propensity score approach helps very little if at all.  A good text on all this (and much more) is Morgan and Winship's [Counterfactuals and Causal Inference: Methods and Principles for Social Research](http://www.cambridge.org/vu/academic/subjects/sociology/sociology-general-interest/counterfactuals-and-causal-inference-methods-and-principles-social-research-2nd-edition?format=PB&isbn=9781107694163).

## Job training example

### Basics

A thorough implementation of various propensity score methods in R comes in [Daniel E. Ho, Kosuke Imai, Gary King, Elizabeth A. Stuart (2011). MatchIt:  Nonparametric Preprocessing for Parametric Causal Inference. Journal of Statistical
Software, Vol. 42, No. 8, pp. 1-28](URL http://www.jstatsoft.org/v42/i08/).  There's a good overview in the [MatchIt vignette](https://cran.r-project.org/web/packages/MatchIt/vignettes/matchit.pdf).  I'll get started with data from one of their examples, which shows a typical application of this technique:

"Our example data set is a subset of the job training program analyzed in Lalonde (1986)
and Dehejia and Wahba (1999). MatchIt includes a subsample of the original data consisting
of the National Supported Work Demonstration (NSW) treated group and the comparison
sample from the Population Survey of Income Dynamics (PSID).1 The variables in
this data set include participation in the job training program (treat, which is equal to 1
if participated in the program, and 0 otherwise), age (age), years of education (educ), race
(black which is equal to 1 if black, and 0 otherwise; hispan which is equal to 1 if hispanic,
and 0 otherwise), marital status (married, which is equal to 1 if married, 0 otherwise), high
school degree (nodegree, which is equal to 1 if no degree, 0 otherwise), 1974 real earnings
(re74), 1975 real earnings (re75), and the main outcome variable, 1978 real earnings (re78)."

First, loading up the functionality I need for the rest of this post
{% highlight R %}
library(tidyverse)
library(forcats)
library(scales)
library(MASS)              # for mvrnorm
library(clusterGeneration) # for genPositiveDefMat
library(boot)              # for inv.logit
library(testthat)          # for expect_equal
library(MatchIt)
library(boot)              # for bootstrapping
library(doParallel)        # for parallel processing
library(foreach)

#==============example from the MatchIt vignette=================
data(lalonde)

# naive comparison - people who got the job training program
# have lower incomes in 1978 - because the training was given
# to people with income problems.  So comparison is unfair:
lalonde %>%
   group_by(treat) %>%
   summarise(Income1978 = mean(re78),
             n = n())
{% endhighlight %}

```
  treat Income1978     n
1     0   6984.170   429
2     1   6349.144   185
```

So we see that the 429 people who didn't get the job training "treatment" had an average income about $635 more than the 185 beneficiaries.  Program failure! 

Obviously that's unfair on the program, so we use `matchit` and `match.data` to create an artificial control group that resembles the treatment group in terms of age, education, ethnicity, marital stats, and income in 1974 and 1975:
{% highlight R %}
# Choose one of the large variety of propensity score matching methods to model propensity
match_model <- matchit(treat ~ age + educ + black + hispan + nodegree + married + re74 + re75, 
                       data = lalonde, method = "nearest")
match_data <- match.data(match_model)
   
# Simple comparison is now much fairer
match_data %>%
   group_by(treat) %>%
   summarise(Income1978 = mean(re78),
             n = n())
{% endhighlight %}

```
  treat Income1978     n
1     0   5440.941   185
2     1   6349.144   185
```
That gives us an average treatment effect of **$908**.  Notice that the new control group is the same size as the treatment group; the rest of the observations have been discarded.

You don't need to limit yourself to simple comparisons, although in principle they should work.  A regression with the matched control and treatment data, even using the same explanatory variables as were used in the matching model, helps address the inevitable lack of complete balance between the two groups.  That gives us a result of **$960** (output not shown).  I use the robust Huber M-estimator to deal with breaches of the classical regression assumptions (particularly the non-Normal response in this case).
{% highlight R %}
# regression model estimate with matched data
coef(MASS::rlm(re78 ~ age + treat + educ + black + hispan + nodegree + married +  re74 + re75, 
        data = match_data))["treat"]
{% endhighlight %}
		
### Regression is simpler and gives similar results		
However, a similar estimate could have come from a simpler, single-step regression with the original data, skipping the propensity score modelling altogether (there are arguments pro and con).  The regression below estimates a treatment effect of **$1,183**. I call this "similar" because the uncertainty around all these estimates is huge, which I'm about to demonstrate.		
{% highlight R %}
# regression model estimate with original data
coef(MASS::rlm(re78 ~ age + treat + educ + black + hispan + nodegree + married +  re74 + re75, 
        data = lalonde))["treat"]
{% endhighlight %}

### Bootstrap or cross-validation must envelope the propensity modelling

The literature has a range of (conflicting) views on estimating uncertainty of statistics estimated after propensity score matching or weighting.  Morgan and Winship report that the bootstrap is now known not to work particularly well in this situation, because the resampling process leaves fewer distinct cases to match to during the propensity modelling stage. However, [Austin and Small reported in 2014](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4260115/) tests of bootstrap methods that seem quite effective.  I don't have time for a really thorough review of this literature, so let's just note that estimating uncertainty after propensity score matching is a moving area of uncertainty itself.

I use the method described by Austin and Small as the "complex bootstrap", which involves resampling from the original data and performing the propensity modelling and matching for each resample.  The propensity modelling is a big source of our uncertainty in the final estimates of interest.  So that source of uncertainty needs to be repeated each time we mimic the sampling process with our bootstrap (the same applies to other pre-processing steps, like imputation).  Austin and Small report that this results in a small overestimate of sampling variability (standard error higher by 7% than it should be; compared to 4% for a simpler bootstrap approach) but my principle is to always include pre-processing inside the bootstrap and I suspect that with less ideal data than Austin and Small use this will pay off.  Anyway, the results reported below look plausible.

Here's code that bootstraps all the three methods of estimating treatment effect above: 

- simple comparison of matched data; 
- regression with matched data; 
- regression with the original data.

{% highlight R %}
#=================bootstrap for confidence intervals=================
# Set up a cluster for parallel computing
cluster <- makeCluster(7) # only any good if you have at least 7 processors :)
registerDoParallel(cluster)

clusterEvalQ(cluster, {
   library(MatchIt)
   data(lalonde)
})

#' Function to estimate treatment effect three different methods
#' @return a vector of three estimates of treatment effect: simple 
#' comparison of matched data; regression with matched data; 
#' regression with the original data.
my_function <- function(x, i){
   resampled_data <- x[i, ]
   match_data <- match.data(
      matchit(treat ~ age + educ + black + hispan + nodegree + married + re74 + re75, 
              data = resampled_data, method = "nearest")
   )
   
   est1 <- with(match_data,
                mean(re78[treat == 1]) -
                   mean(re78[treat == 0]))
   
   
   # regression model estimate with matched data
   est2 <- coef(lm(re78 ~ treat + age + educ + black + hispan + nodegree + married +  re74 + re75, 
           data = match_data))["treat"]
   
   # regression model estimate with original data
   est3 <- coef(lm(re78 ~ treat + age + educ + black + hispan + nodegree + married +  re74 + re75, 
                   data = resampled_data))["treat"]
   return(c(est1, est2, est3))
}

booted <- boot(lalonde, statistic = my_function, R = 5000, 
     parallel = "snow", cl = cluster)

booted_df <- as.data.frame(booted$t)
names(booted_df) <- c("Simple with matched data", "Regression with matched data", "Regression with original data")
booted_df %>%
   gather("Method", "Estimate") %>%
   mutate(Method = fct_reorder(Method, Estimate)) %>%
   ggplot(aes(x = Estimate, fill = Method)) +
   geom_density(alpha = 0.4, colour = "grey50") +
   scale_x_continuous(label = dollar) +
   ggtitle("Uncertainty of estimates of treatment effects from three different methods",
           "Estimated impact on income in 1978 of a job training program") +
   labs(x = "Estimated treatment effect",
        caption = "Lalonde's 1986 data on a job training program",
        fill = "")

# biases:
booted

# confidence intervals:
boot.ci(booted, index = 1, type = "perc")
boot.ci(booted, index = 2, type = "perc")
boot.ci(booted, index = 3, type = "perc")
{% endhighlight %}

<img src='/img/0090-job-treatment.svg' width = '100%'>

```
> booted
Bootstrap Statistics :
     original       bias    std. error
t1*  894.3675 -249.0077691    743.6632
t2* 1344.9356  -85.3633228    755.4421
t3* 1548.2438    0.5805344    746.3255
> 
> # confidence intervals:
> boot.ci(booted, index = 1, type = "perc")
Level     Percentile     
95%   (-783.1, 2126.4 )  

> boot.ci(booted, index = 2, type = "perc")
Level     Percentile     
95%   (-222, 2764 )  

> boot.ci(booted, index = 3, type = "perc")
Level     Percentile     
95%   ( 104, 3045 )  
```

Those are big confidence intervals - reflecting the small sample size and the difficulty of picking up an impact of an intervention amongst all the complexities of income determinants.

### Weighting might be better than matching

Creating a control group by matching has the distressing side-effect of throwing away large amounts of the data, because the control group is shrunk down to the same size as the treatment group.  A possibly better use of the propensity scores is to keep all observations in play but weight them according to the propensity score - one of the methods described by Peter Austin in [this article on "An introduction to propensity score methods for reducing the effects of confounding in observational studies"](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3144483/).  

Under "inverse probability of treatment weighting", proposed by Imbens in 2000, observations that receive the treatment are given weight of $$\frac{1}{p}$$ and those that did not receive the treatment are given weight of $$\frac{1}{1-p}$$, where p is the probability of getting the treatment.  That is, each observation is given weight of the inverse of the probability of the treatment they actually got.  Intuitively, treatment cases that resemble the controls are interesting and given more weight, and control cases that look like they should have got the treatment are interesting and get more weight.  Analysis can then proceed via average values, or a regression with explanatory variables (which may or may not be the same as those used in the model of propensity for treatment).

Note however an [incomplete article by Posner and Ash](http://www.stat.columbia.edu/~gelman/stuff_for_blog/posner.pdf) (there may be a complete version somewhere else) points out some problems with this method:

> "While this method can be shown to have nice mathematical properties, it does not work well in practice. Consider a lone treated observation that happens to have a very low probability of being treated.... The value of the inverse of the propensity score will be extremely high, asymptotically infinity. The effect size obtained will be dominated by this single value, and any fluctuations in it will produce wildly varied results, which is an undesirable property."

Ouch.  Posner and Ash go on to suggest alternative ways of weighting less vulnerable to these problems.

Anyway, Ho et al (2007) suggest that (in Morgan and Winship's paraphrase):
> "the general procedure one should carry out in any multivariate analysis that aspires to generate causal inferences is to first balance one's data as much as possible with a matching routine and then estimate a regression model on the matched data.  From this perspective, matching is a preprocessor, which can be used to prepare the data for subsequent analysis with something such as a regression model."

The "doubly robust" approach to regression modelling to identify causal effects uses weights from propensity score matching as well as a regression.  

> "There has been considerable debate as to which approach to confounder control is to be preferred, as the first is biased if the outcome regression model is misspecified while the second approach is biased if the treatment regression, ie propensity, model is misspecified.  This controversy could be resolved if an estimator were available that was guaranteed to be consistent ... whenever at least one of the two models was correct. ... We refer to such combined methods as doubly-robust or doubly-protected as they can protect against misspecification of either the outcome or treatment model, although not against simultaneous misspecification of both." 

*Robins and Rotnitzky 2001, quoted in Morgan and Winship Counterfactuals and Causal Analysis*

These weighting methods seem better than simply matching, if only because they retain all the data, but crucially they are still vulnerable to model mis-specification and unobserved explanatory variables. There's a good [critical discussion in this article by Freedman and Berk](https://www.stat.berkeley.edu/~census/weight.pdf)

> "Regressions can be weighted by propensity scores in order to reduce bias. However,
weighting is likely to increase random error in the estimates, and to bias the
estimated standard errors downward, even when selection mechanisms are well understood.
Moreover, in some cases, weighting will increase the bias in estimated
causal parameters. If investigators have a good causal model, it seems better just to
fit the model without weights. If the causal model is improperly specified, there can
be significant problems in retrieving the situation by weighting, although weighting
may help under some circumstances."

And with respect to the "doubly robust" approach:
> "you need to get at least one of the two models (and preferably both) nearly right in order for weighting to help much. If both models are wrong, weighting could easily be a dead end. There are papers suggesting that under some circumstances, estimating a shaky causal model and a shaky selection model should be doubly robust. Our results indicate that under other circumstances, the technique is doubly frail."

When I see multi-stage approaches like propensity score matching or weighting - just like structural equation models, and two or three stage least squares - that aim to deal with causality by adding complexity, I always get very nervous; the more so when I read criticisms like those above.

## "Roll your own" nearest neighbour matching

To be confident I understood how things were working under the hood I wanted to do some of my own propensity score matching rather than relying purely on `MatchIt`.  There is a big range of options for how the matching is done once you have a set of propensity scores, and an intensely uninteresting debate about their various merits.  I chose to implement one of the cruder methods, nearest neighbour with replacement; this simply matches each treatment observation with the non-treatment observation that has the probability of treatment that is closest to its own, most commonly implemented on the logit scale.

Here's my implementation of that.

{% highlight R %}
#============roll your own nearest neighbour matching with replacement===========
#' Helper function for matching a number to a vector
#' @param x1 single number
#' @param x2 vector
#' @value index indicating which element of x2 is closest to x1
min_diff = function(x1, x2){ 
   if(length(x1) != 1){stop("x1 should be a single number")}
   which.min(abs(x1 - x2)) 
} 
expect_equal(min_diff(4.3, 1:10), 4)

model <- glm(treat ~ age + educ + black + hispan + nodegree + married +  re74 + re75, 
             data = lalonde, family = "binomial")
# Compare this to the model used by matchit earlier.  Identical:
cbind(coef(match_model$model), coef(model))

lalonde$prop_scores <- predict(model, type = "link")

lalonde %>%
   mutate(treat = as.factor(treat)) %>%
   ggplot(aes(x = prop_scores, fill = treat))  + 
   geom_density(alpha = 0.5) +
   ggtitle("Propensity for getting the treatment",
           "split by whether the subject actually did get the treatment or not") +
   labs(x = "Propensity to get the treatment (logit scale, so 0 means 50% chance)",
        caption = "Lalonde's 1986 data on a job training program")


untreated_group <- filter(lalonde, treat == 0)
treatment_group <- filter(lalonde, treat == 1)

matches <- sapply(treatment_group$prop_scores, min_diff, untreated_group$prop_scores)
expect_equal(length(matches), nrow(treatment_group))

control_group <- untreated_group[matches, ]
{% endhighlight %}

If I were doing this for real, I would have considered using a generalized additive model so there could be non-linear relationships between age, previous income and treatment; but I wanted to use exactly the same model as MatchIt.

Here are the first three treatment-control pairs of observations
```
> rbind(control_group[1, ], treatment_group[1, ])
   treat age educ black hispan married nodegree     re74     re75      re78 prop_scores
69     0  30   17     1      0       0        0 17827.37 5546.419 14421.130   0.5690943
1      1  37   11     1      0       1        1     0.00    0.000  9930.046   0.5700293
> rbind(control_group[2, ], treatment_group[2, ])
    treat age educ black hispan married nodegree     re74     re75     re78 prop_scores
111     0  51   11     0      0       0        1 48.98167 3813.387 1525.014   -1.242050
2       1  22    9     0      1       0        1  0.00000    0.000 3595.894   -1.238861
> rbind(control_group[3, ], treatment_group[3, ])
    treat age educ black hispan married nodegree re74 re75      re78 prop_scores
370     0  16    9     1      0       0        1    0    0  2158.959   0.7482103
3       1  30   12     1      0       0        0    0    0 24909.450   0.7457131
```

# This code compares my results with MatchIt's.  The results aren't interesting so I don't show them.

{% highlight R %}
# Compare the two groups:
c(mean(control_group$re78), mean(treatment_group$re78))
# Or by removing the duplicates from the control group where more than one
# treatment matched (because this seems to be how MatchIt does it):
c(mean(distinct(control_group)$re78), mean(treatment_group$re78))

# Choose one of the large variety of propensity score matching methods to model propensity
# Note - there is some randomness here, and using set.seed() doesn't make it reproducible:
matchit(treat ~ age + educ + black + hispan + nodegree + married + re74 + re75, 
                       data = lalonde, method = "nearest", replace = TRUE) %>%
   match.data() %>%
   group_by(treat) %>%
   summarise(Income1978 = mean(re78),
             n = n())
{% endhighlight %}

## Omitted variable bias

{% highlight R %}


{% endhighlight %}

{% highlight R %}


{% endhighlight %}




