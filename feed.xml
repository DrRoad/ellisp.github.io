<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/">
	<channel>
		<title>free range statistics</title>
		<description>I write about applications of data and analytical techniques like statistical modelling and simulation to real-world situations. I show how to access and use data, and provide examples of analytical products and the code that produced them.</description>		
		<link>http://freerangestats.info</link>
		<atom:link href="http://freerangestats.info/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Lewis Carroll&#39;s proposed rules for tennis tournaments</title>
				        
				
				
					<description>I put to the test a method of running a tennis tournament suggested by Lewis Carroll. It performs ok in allocating prizes fairly, although it takes about twice as many matches as a standard modern single-elimination. When there is realistic randomness in results it doesn&#39;t perform as well as Carroll argued it would on the unrealistic basis of deterministic match outcomes.</description>
				
				
				<pubDate>Sat, 01 Feb 2020 00:00:00 +1100</pubDate>
				<link>http://freerangestats.info/blog/2020/02/01/tennis-carroll</link>
				<guid isPermaLink="true">http://freerangestats.info/blog/2020/02/01/tennis-carroll</guid>
				
				  
				
			</item>
		
			<item>
				<title>Analysing the effectiveness of tennis tournament seeding</title>
				        
				
				
					<description>I have a go at quantifying how much giving a special draw to the top 32 seeds in a tennis tournament impacts on who makes it to the finals and who wins, based on simulations of a hypothetical matchup of the 128 top women players in 1990.</description>
				
				
				<pubDate>Sun, 26 Jan 2020 00:00:00 +1100</pubDate>
				<link>http://freerangestats.info/blog/2020/01/26/tennis-seeding</link>
				<guid isPermaLink="true">http://freerangestats.info/blog/2020/01/26/tennis-seeding</guid>
				
				  
				
			</item>
		
			<item>
				<title>Analysing large data on your laptop with a database and R</title>
				        
				
				
					<description>SQL Server and R work fine together for analysing 200 GB of the New York City taxi data. There&#39;s a lot of effort needed to prepare for analysis even relatively-tidy data. Also, you can&#39;t analyse big data without aggregating and summarising it somehow.</description>
				
				
				<pubDate>Sun, 22 Dec 2019 00:00:00 +1100</pubDate>
				<link>http://freerangestats.info/blog/2019/12/22/nyc-taxis-sql</link>
				<guid isPermaLink="true">http://freerangestats.info/blog/2019/12/22/nyc-taxis-sql</guid>
				
				  
				
			</item>
		
			<item>
				<title>Cost-benefit analysis in R</title>
				        
				
				
					<description>I try to show that cost-benefit analysis is easy to perform in R, and that R lets you build in uncertainty in a much clearer way than is generally done; and to demystify the internal rate of return.</description>
				
				
				<pubDate>Sun, 24 Nov 2019 00:00:00 +1100</pubDate>
				<link>http://freerangestats.info/blog/2019/11/24/cost-benefit-analysis</link>
				<guid isPermaLink="true">http://freerangestats.info/blog/2019/11/24/cost-benefit-analysis</guid>
				
				  
				
			</item>
		
			<item>
				<title>A small simple random sample will often be better than a huge not-so-random one</title>
				        
				
				
					<description>A small random sample will give better results than a much larger non-random sample, under certain conditions; but more importantly, it is reliable and controls for risk.</description>
				
				
				<pubDate>Sat, 09 Nov 2019 00:00:00 +1100</pubDate>
				<link>http://freerangestats.info/blog/2019/11/09/sampling-from-urns</link>
				<guid isPermaLink="true">http://freerangestats.info/blog/2019/11/09/sampling-from-urns</guid>
				
				  
				
			</item>
		
			<item>
				<title>Re-creating survey microdata from marginal totals</title>
				        
				
				
					<description>I play around with creating my own synthetic unit record file of survey microdata to match published marginal totals. I conclude that making synthetic data for development or education purposes can be useful, but the effort to try to exactly match marginal totals from this sort of survey is unlikely to be useful for either bona fide researchers or ill-intentioned data snoopers.</description>
				
				
				<pubDate>Sun, 03 Nov 2019 00:00:00 +1100</pubDate>
				<link>http://freerangestats.info/blog/2019/11/03/re-creating-microdata</link>
				<guid isPermaLink="true">http://freerangestats.info/blog/2019/11/03/re-creating-microdata</guid>
				
				  
				
			</item>
		
			<item>
				<title>Poisson point processes, mass shootings and clumping</title>
				        
				
				
					<description>I annotate and explain an example use of Poisson process modelling to test an important hypothesis about the frequency of mass shootings in Australia over time.</description>
				
				
				<pubDate>Sat, 07 Sep 2019 00:00:00 +1000</pubDate>
				<link>http://freerangestats.info/blog/2019/09/07/mass-shootings-oz</link>
				<guid isPermaLink="true">http://freerangestats.info/blog/2019/09/07/mass-shootings-oz</guid>
				
				  
				
			</item>
		
			<item>
				<title>Inferring a continuous distribution from binned data</title>
				        
				
				
					<description>I show how modelling the distribution of an underlying continuous variable that has been clouded by binning is much better way of understanding the data than crude methods dealing directly with the binned counts.</description>
				
				
				<pubDate>Sun, 25 Aug 2019 00:00:00 +1000</pubDate>
				<link>http://freerangestats.info/blog/2019/08/25/fitting-bins</link>
				<guid isPermaLink="true">http://freerangestats.info/blog/2019/08/25/fitting-bins</guid>
				
				  
				
			</item>
		
			<item>
				<title>Forecasting unemployment</title>
				        
				
				
					<description>Forecasting unemployment is hard, with lots of complex bi-directional causality. Also, while AIC is asymptotically equivalent to cross-validation, it&#39;s probably better to check. It turns out that interest rates or stock prices don&#39;t have any useful information for nowcasting unemployment.</description>
				
				
				<pubDate>Sun, 28 Jul 2019 00:00:00 +1000</pubDate>
				<link>http://freerangestats.info/blog/2019/07/28/unemployment-forecasts</link>
				<guid isPermaLink="true">http://freerangestats.info/blog/2019/07/28/unemployment-forecasts</guid>
				
				  
				
			</item>
		
			<item>
				<title>Time series forecast cross-validation</title>
				        
				
				
					<description>I use time series forecast cross-validation to explore simulated data and sort out the real and the fake effects between variables.</description>
				
				
				<pubDate>Sat, 20 Jul 2019 00:00:00 +1000</pubDate>
				<link>http://freerangestats.info/blog/2019/07/20/time-series-cv</link>
				<guid isPermaLink="true">http://freerangestats.info/blog/2019/07/20/time-series-cv</guid>
				
				  
				
			</item>
		
	</channel>
</rss>